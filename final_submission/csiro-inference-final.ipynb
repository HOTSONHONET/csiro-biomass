{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25874916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:16:17.512665Z",
     "iopub.status.busy": "2026-01-27T22:16:17.512370Z",
     "iopub.status.idle": "2026-01-27T22:16:21.403545Z",
     "shell.execute_reply": "2026-01-27T22:16:21.402504Z"
    },
    "papermill": {
     "duration": 3.899124,
     "end_time": "2026-01-27T22:16:21.405440",
     "exception": false,
     "start_time": "2026-01-27T22:16:17.506316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/timm-1-0-22/timm-1.0.22-py3-none-any.whl\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/timm-1-0-22/timm-1.0.22-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/timm-1-0-22/timm-1.0.22-py3-none-any.whl --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c5a36",
   "metadata": {
    "papermill": {
     "duration": 0.004088,
     "end_time": "2026-01-27T22:16:21.413883",
     "exception": false,
     "start_time": "2026-01-27T22:16:21.409795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8e840c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-27T22:16:21.423870Z",
     "iopub.status.busy": "2026-01-27T22:16:21.423163Z",
     "iopub.status.idle": "2026-01-27T22:17:12.454086Z",
     "shell.execute_reply": "2026-01-27T22:17:12.453443Z"
    },
    "papermill": {
     "duration": 51.038015,
     "end_time": "2026-01-27T22:17:12.455852",
     "exception": false,
     "start_time": "2026-01-27T22:16:21.417837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from warnings import filterwarnings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from dataclasses import dataclass\n",
    "import gc\n",
    "\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be816ac0",
   "metadata": {
    "papermill": {
     "duration": 0.004184,
     "end_time": "2026-01-27T22:17:12.464330",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.460146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6929e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:17:12.475198Z",
     "iopub.status.busy": "2026-01-27T22:17:12.474676Z",
     "iopub.status.idle": "2026-01-27T22:17:12.543414Z",
     "shell.execute_reply": "2026-01-27T22:17:12.542762Z"
    },
    "papermill": {
     "duration": 0.076353,
     "end_time": "2026-01-27T22:17:12.544738",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.468385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed to  42\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    test_dir: Path = Path(\"/kaggle/input/csiro-biomass/test\")\n",
    "    train_dir: Path = Path(\"/kaggle/input/csiro-biomass/train\")\n",
    "    train_csv_path: Path = Path(\"/kaggle/input/csiro-biomass/train.csv\")\n",
    "    seed: int = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(\"set seed to \", seed)\n",
    "\n",
    "set_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74612c5b",
   "metadata": {
    "papermill": {
     "duration": 0.003932,
     "end_time": "2026-01-27T22:17:12.552794",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.548862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0485028",
   "metadata": {
    "papermill": {
     "duration": 0.003846,
     "end_time": "2026-01-27T22:17:12.560521",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.556675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Mamba Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317a56e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:17:12.570144Z",
     "iopub.status.busy": "2026-01-27T22:17:12.569582Z",
     "iopub.status.idle": "2026-01-27T22:17:12.576116Z",
     "shell.execute_reply": "2026-01-27T22:17:12.575362Z"
    },
    "papermill": {
     "duration": 0.013268,
     "end_time": "2026-01-27T22:17:12.577658",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.564390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalMambaBlock(nn.Module):\n",
    "    def __init__(self, dim: int, kernal_size: int = 5, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.dwconv = nn.Conv1d(\n",
    "            dim, dim,\n",
    "            kernel_size=kernal_size,\n",
    "            padding=kernal_size // 2,\n",
    "            groups=dim  # depthwise conv\n",
    "        )\n",
    "        self.gate = nn.Linear(dim, dim)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, D)\n",
    "        shortcut = x\n",
    "        x = self.norm(x)\n",
    "        g = torch.sigmoid(self.gate(x))  # (B,T,D)\n",
    "        x = x * g\n",
    "\n",
    "        x = x.transpose(1, 2)            # (B,D,T)\n",
    "        x = self.dwconv(x)               # (B,D,T)\n",
    "        x = x.transpose(1, 2)            # (B,T,D)\n",
    "\n",
    "        x = self.proj(x)                 # (B,T,D)\n",
    "        x = self.dropout(x)\n",
    "        return shortcut + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1694c1b1",
   "metadata": {
    "papermill": {
     "duration": 0.004123,
     "end_time": "2026-01-27T22:17:12.586107",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.581984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dinov3MultiReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c61806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:17:12.596009Z",
     "iopub.status.busy": "2026-01-27T22:17:12.595681Z",
     "iopub.status.idle": "2026-01-27T22:17:12.612753Z",
     "shell.execute_reply": "2026-01-27T22:17:12.612107Z"
    },
    "papermill": {
     "duration": 0.02395,
     "end_time": "2026-01-27T22:17:12.614190",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.590240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dinov3Config:\n",
    "    model_id: str = \"facebook/dinov3-vith16plus-pretrain-lvd1689m\"  # HF (may be gated)\n",
    "    timm_id: str = \"vit_huge_plus_patch16_dinov3.lvd1689m\"           # fallback timm\n",
    "    patch_size: int = 16\n",
    "\n",
    "    # tiling setup: 2x4 = 8 tiles\n",
    "    tile_rows: int = 2\n",
    "    tile_cols: int = 4\n",
    "    tile_size: int = 512   # S (must be divisible by patch_size)\n",
    "\n",
    "\n",
    "class Dinov3MultiReg(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      img_full: (B, 3, H, W) where:\n",
    "        H = tile_rows * tile_size\n",
    "        W = tile_cols * tile_size\n",
    "      Example for 2x4 tiles: H=2S, W=4S\n",
    "\n",
    "    Internally:\n",
    "      tiles: (B, N, 3, S, S) where N=tile_rows*tile_cols\n",
    "      encode tiles in one shot: (B*N, T, D)\n",
    "      reshape + concat tokens: (B, N*T, D)\n",
    "      fusion -> pool -> heads -> (B,5)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Dinov3Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.backend = None\n",
    "        self.encoder = None\n",
    "\n",
    "        self.encoder = timm.create_model(cfg.timm_id, pretrained=False)\n",
    "        self.backend = \"timm\"\n",
    "\n",
    "        # Freeze backbone\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.encoder.eval()\n",
    "\n",
    "        # Figure out D\n",
    "        D = getattr(self.encoder, \"num_features\", None)\n",
    "        if D is None:\n",
    "            if self.backend == \"hf\":\n",
    "                D = self.encoder.config.hidden_size\n",
    "            else:\n",
    "                # timm usually has num_features\n",
    "                D = getattr(self.encoder, \"num_features\", None)\n",
    "                if D is None:\n",
    "                    raise RuntimeError(\"Could not infer hidden size D from timm model.\")\n",
    "        self.num_features = D\n",
    "\n",
    "        print(\n",
    "            f\"Encoder trainable params: {sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)}\"\n",
    "            f\" | backend: {self.backend} | D: {self.num_features}\"\n",
    "        )\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            LocalMambaBlock(self.num_features, kernal_size=5, dropout=0.1),\n",
    "            LocalMambaBlock(self.num_features, kernal_size=5, dropout=0.1),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.head_green = nn.Sequential(\n",
    "            nn.Linear(self.num_features, self.num_features // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.num_features // 2, 1),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "        self.head_dead = nn.Sequential(\n",
    "            nn.Linear(self.num_features, self.num_features // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.num_features // 2, 1),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "        self.head_clover = nn.Sequential(\n",
    "            nn.Linear(self.num_features, self.num_features // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.num_features // 2, 1),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode_tokens(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B,3,S,S)\n",
    "        returns tokens: (B,T,D)\n",
    "        \"\"\"\n",
    "        if self.backend == \"hf\":\n",
    "            out = self.encoder(pixel_values=x)\n",
    "            return out.last_hidden_state  # (B,T,D)\n",
    "\n",
    "        # timm path: prefer forward_features\n",
    "        if hasattr(self.encoder, \"forward_features\"):\n",
    "            feats = self.encoder.forward_features(x)\n",
    "        else:\n",
    "            feats = self.encoder(x)\n",
    "\n",
    "        # Many timm ViTs return (B,T,D). Some return (B,D).\n",
    "        if feats.ndim == 3:\n",
    "            return feats\n",
    "        if feats.ndim == 2:\n",
    "            # fallback: treat as pooled embedding, make it look like one token\n",
    "            return feats.unsqueeze(1)  # (B,1,D)\n",
    "        raise RuntimeError(f\"Unexpected timm features shape: {feats.shape}\")\n",
    "\n",
    "    def _tile(self, img_full: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        img_full: (B,3,H,W) where H=R*S, W=C*S\n",
    "        returns tiles: (B,N,3,S,S)\n",
    "        \"\"\"\n",
    "        B, C, H, W = img_full.shape\n",
    "        R, Cc, S = self.cfg.tile_rows, self.cfg.tile_cols, self.cfg.tile_size\n",
    "\n",
    "        expected_h = R * S\n",
    "        expected_w = Cc * S\n",
    "        if H != expected_h or W != expected_w:\n",
    "            raise AssertionError(\n",
    "                f\"Expected img_full H,W=({expected_h},{expected_w}) \"\n",
    "                f\"for tile_rows={R}, tile_cols={Cc}, tile_size={S}, \"\n",
    "                f\"but got ({H},{W}). Ensure your dataset resize matches.\"\n",
    "            )\n",
    "\n",
    "        # reshape to grid then flatten tiles\n",
    "        # (B,3,R,S,Cc,S) -> (B,R,Cc,3,S,S) -> (B,N,3,S,S)\n",
    "        x = img_full.view(B, 3, R, S, Cc, S)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5).contiguous()\n",
    "        tiles = x.view(B, R * Cc, 3, S, S)\n",
    "        return tiles\n",
    "\n",
    "    def forward(self, img_full: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        img_full: (B,3,2S,4S) for 2x4 tiles\n",
    "        \"\"\"\n",
    "        # ensure backbone stays deterministic even when model.train()\n",
    "        self.encoder.eval()\n",
    "\n",
    "        tiles = self._tile(img_full)  # (B,N,3,S,S)\n",
    "        B, N, C, S, S2 = tiles.shape  # S2 == S\n",
    "\n",
    "        # Encode all tiles together\n",
    "        tiles_flat = tiles.view(B * N, 3, S, S)         # (B*N,3,S,S)\n",
    "        tok = self._encode_tokens(tiles_flat)           # (B*N,T,D)\n",
    "\n",
    "        # reshape back + concat tokens across tiles\n",
    "        T = tok.shape[1]\n",
    "        tok = tok.view(B, N * T, self.num_features)     # (B, N*T, D)\n",
    "\n",
    "        # fuse + pool\n",
    "        tok = self.fusion(tok)                          # (B, N*T, D)\n",
    "        pooled = self.pool(tok.transpose(1, 2)).flatten(1)  # (B,D)\n",
    "\n",
    "        green = self.head_green(pooled)   # (B,1)\n",
    "        dead = self.head_dead(pooled)     # (B,1)\n",
    "        clover = self.head_clover(pooled) # (B,1)\n",
    "\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        return torch.cat([green, dead, clover, gdm, total], dim=1)  # (B,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aba3c7",
   "metadata": {
    "papermill": {
     "duration": 0.003868,
     "end_time": "2026-01-27T22:17:12.622291",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.618423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### DinoV3Structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd72429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:17:12.631350Z",
     "iopub.status.busy": "2026-01-27T22:17:12.631126Z",
     "iopub.status.idle": "2026-01-27T22:17:12.645285Z",
     "shell.execute_reply": "2026-01-27T22:17:12.644641Z"
    },
    "papermill": {
     "duration": 0.020591,
     "end_time": "2026-01-27T22:17:12.646724",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.626133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DinoV3StructuredConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"vit_huge_plus_patch16_dinov3.lvd1689m\",\n",
    "        img_size: int = 512,\n",
    "        tiles: int = 2,          # 2 = left/right, 8 = your tile idea\n",
    "        freeze_backbone: bool = True,\n",
    "        dropout: float = 0.1,\n",
    "        use_mamba: bool = True,\n",
    "        mamba_depth: int = 2,\n",
    "    ):\n",
    "        self.timm_id = model_id\n",
    "        self.img_size = img_size\n",
    "        self.tiles = tiles\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "        self.dropout = dropout\n",
    "        self.use_mamba = use_mamba\n",
    "        self.mamba_depth = mamba_depth\n",
    "\n",
    "\n",
    "class DinoV3Structured(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: full image tensor (B,3,H,W) already resized consistently (e.g. 2S x 4S).\n",
    "    We tile inside the model into N tiles, encode each tile with frozen timm ViT,\n",
    "    fuse tile embeddings, then output structured predictions.\n",
    "\n",
    "    Outputs:\n",
    "      dict with:\n",
    "        green_pred, total_pred, dead_ratio_pred, clover_ratio_pred\n",
    "        dead_pred, gdm_pred, clover_pred\n",
    "        pred5 in order [Green, Dead, Clover, GDM, Total]\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: DinoV3StructuredConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # timm backbone (offline ok if weights are available; else set pretrained=False)\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg.timm_id,\n",
    "            pretrained=False,\n",
    "            num_classes=0,            # removes classification head\n",
    "            global_pool=\"avg\",        # gives (B, D)\n",
    "        )\n",
    "\n",
    "        self.D = self.encoder.num_features\n",
    "\n",
    "        if cfg.freeze_backbone:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "            self.encoder.eval()\n",
    "\n",
    "        # Fusion (tile tokens -> one vector)\n",
    "        # You can run Mamba blocks on (B, T, D) then pool.\n",
    "        if cfg.use_mamba:\n",
    "            blocks = []\n",
    "            for _ in range(cfg.mamba_depth):\n",
    "                blocks.append(LocalMambaBlock(self.D, kernal_size=5, dropout=cfg.dropout))\n",
    "            self.fusion = nn.Sequential(*blocks)\n",
    "        else:\n",
    "            self.fusion = nn.Identity()\n",
    "\n",
    "        self.tile_pool = nn.AdaptiveAvgPool1d(1)  # (B, D, T) -> (B, D, 1)\n",
    "\n",
    "        # Heads\n",
    "        # 1) regression heads (green, total) — force non-negative via softplus\n",
    "        self.head_reg = nn.Sequential(\n",
    "            nn.LayerNorm(self.D),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(self.D, self.D // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.D // 2, 2),  # [green_raw, total_raw]\n",
    "        )\n",
    "\n",
    "        # 2) ratio heads (dead_ratio, clover_ratio) — sigmoid\n",
    "        self.head_ratio = nn.Sequential(\n",
    "            nn.LayerNorm(self.D),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(self.D, self.D // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.D // 2, 2),  # [dead_ratio_logit, clover_ratio_logit]\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # Tiling utilities\n",
    "    # -------------------------\n",
    "    def _make_tiles(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B,3,H,W)\n",
    "        returns tiles: (B*T, 3, tileH, tileW)\n",
    "        Supported:\n",
    "          tiles=2  -> split width into left/right\n",
    "          tiles=8  -> 2 rows x 4 cols grid\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        if self.cfg.tiles == 2:\n",
    "            mid = W // 2\n",
    "            left = x[:, :, :, :mid]\n",
    "            right = x[:, :, :, mid:]\n",
    "            tiles = torch.cat([left, right], dim=0)  # (2B,3,H,W/2)\n",
    "            return tiles\n",
    "\n",
    "        if self.cfg.tiles == 8:\n",
    "            # 2x4 grid\n",
    "            rows, cols = 2, 4\n",
    "            th, tw = H // rows, W // cols\n",
    "            tile_list = []\n",
    "            for r in range(rows):\n",
    "                for c in range(cols):\n",
    "                    tile = x[:, :, r*th:(r+1)*th, c*tw:(c+1)*tw]\n",
    "                    tile_list.append(tile)\n",
    "            tiles = torch.cat(tile_list, dim=0)  # (B*8, 3, th, tw)\n",
    "            return tiles\n",
    "\n",
    "        raise ValueError(f\"Unsupported tiles={self.cfg.tiles}. Use 2 or 8.\")\n",
    "\n",
    "    def _encode_tiles(self, tiles: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        tiles: (B*T,3,tH,tW) -> embeddings (B*T,D)\n",
    "        \"\"\"\n",
    "        if self.cfg.freeze_backbone:\n",
    "            with torch.no_grad():\n",
    "                z = self.encoder(tiles)  # (B*T, D)\n",
    "        else:\n",
    "            z = self.encoder(tiles)\n",
    "        return z\n",
    "\n",
    "    # -------------------------\n",
    "    # Forward\n",
    "    # -------------------------\n",
    "    def forward(self, x_full: torch.Tensor) -> dict:\n",
    "        B = x_full.size(0)\n",
    "        T = self.cfg.tiles\n",
    "\n",
    "        tiles = self._make_tiles(x_full)          # (B*T,3,th,tw)\n",
    "        z = self._encode_tiles(tiles)             # (B*T, D)\n",
    "\n",
    "        # reshape to (B, T, D)\n",
    "        z = z.view(T, B, self.D).transpose(0, 1)  # (B, T, D)\n",
    "\n",
    "        # fusion expects (B, T, D) for mamba blocks (you already used that pattern)\n",
    "        z = self.fusion(z)                        # (B, T, D)\n",
    "\n",
    "        # pool over T -> (B, D)\n",
    "        z = z.transpose(1, 2)                     # (B, D, T)\n",
    "        z = self.tile_pool(z).squeeze(-1)         # (B, D)\n",
    "\n",
    "        # heads\n",
    "        reg_raw = self.head_reg(z)                # (B,2)\n",
    "        ratio_logit = self.head_ratio(z)          # (B,2)\n",
    "\n",
    "        green = F.softplus(reg_raw[:, 0])         # >=0\n",
    "        total = F.softplus(reg_raw[:, 1])         # >=0\n",
    "\n",
    "        # optional: enforce total >= green softly\n",
    "        # total = green + F.softplus(reg_raw[:, 1])\n",
    "\n",
    "        dead_ratio = torch.sigmoid(ratio_logit[:, 0])    # (0,1)\n",
    "        clover_ratio = torch.sigmoid(ratio_logit[:, 1])  # (0,1)\n",
    "\n",
    "        dead = dead_ratio * total\n",
    "        gdm = total - dead\n",
    "        clover = clover_ratio * gdm\n",
    "\n",
    "        pred5 = torch.stack([green, dead, clover, gdm, total], dim=1)\n",
    "\n",
    "        return {\n",
    "            \"green_pred\": green,\n",
    "            \"total_pred\": total,\n",
    "            \"dead_ratio_pred\": dead_ratio,\n",
    "            \"clover_ratio_pred\": clover_ratio,\n",
    "            \"dead_pred\": dead,\n",
    "            \"gdm_pred\": gdm,\n",
    "            \"clover_pred\": clover,\n",
    "            \"pred5\": pred5,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f434ff",
   "metadata": {
    "papermill": {
     "duration": 0.003765,
     "end_time": "2026-01-27T22:17:12.655042",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.651277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### DinoV3Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6234e6bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:17:12.663781Z",
     "iopub.status.busy": "2026-01-27T22:17:12.663572Z",
     "iopub.status.idle": "2026-01-27T22:17:12.678390Z",
     "shell.execute_reply": "2026-01-27T22:17:12.677756Z"
    },
    "papermill": {
     "duration": 0.020917,
     "end_time": "2026-01-27T22:17:12.679758",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.658841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class DinoV3HybridConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id=\"vit_huge_plus_patch16_dinov3.lvd1689m\",\n",
    "        pretrained_backbone=False,\n",
    "        dropout=0.2,\n",
    "        mamba_depth=2,\n",
    "        use_grad_checkpointing=False,\n",
    "        freeze_backbone=True,\n",
    "        # blending\n",
    "        init_mix_logits=(-1.0, -1.0),  # (dead_mix, clover_mix) negative => prefer derived early\n",
    "    ):\n",
    "        self.model_id = model_id\n",
    "        self.pretrained_backbone = pretrained_backbone\n",
    "        self.dropout = dropout\n",
    "        self.mamba_depth = mamba_depth\n",
    "        self.use_grad_checkpointing = use_grad_checkpointing\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "        self.init_mix_logits = init_mix_logits\n",
    "\n",
    "\n",
    "class DinoV3Hybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    Token-level fusion (left/right), separate heads + ratio heads.\n",
    "\n",
    "    Direct heads:\n",
    "      - green_raw, gdm_raw, total_raw  (Softplus)\n",
    "      - optionally dead_raw, clover_raw (Softplus)  [kept here]\n",
    "\n",
    "    Ratio heads:\n",
    "      - dead_ratio in (0,1)\n",
    "      - clover_ratio in (0,1)\n",
    "\n",
    "    Derived:\n",
    "      - dead_from_core   = relu(total - gdm)\n",
    "      - clover_from_core = relu(gdm - green)\n",
    "      - dead_from_ratio  = dead_ratio * total\n",
    "      - clover_from_ratio= clover_ratio * gdm\n",
    "\n",
    "    Final:\n",
    "      dead   = mix_dead * dead_direct + (1-mix_dead) * dead_from_core   (or ratio)\n",
    "      clover = mix_clover * clover_direct + (1-mix_clover) * clover_from_core (or ratio)\n",
    "\n",
    "    Outputs pred5: [Green, Dead, Clover, GDM, Total]\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: DinoV3HybridConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # keep patch tokens: (B, N, D)\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_id,\n",
    "            pretrained=cfg.pretrained_backbone,\n",
    "            num_classes=0,\n",
    "            global_pool=\"\",  # IMPORTANT: keep tokens\n",
    "        )\n",
    "\n",
    "        if hasattr(self.backbone, \"set_grad_checkpointing\") and cfg.use_grad_checkpointing:\n",
    "            self.backbone.set_grad_checkpointing(True)\n",
    "\n",
    "        self.D = self.backbone.num_features\n",
    "\n",
    "        if cfg.freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "            self.backbone.eval()\n",
    "\n",
    "        # Mamba fusion over token sequence\n",
    "        blocks = []\n",
    "        for _ in range(cfg.mamba_depth):\n",
    "            blocks.append(LocalMambaBlock(self.D, kernal_size=5, dropout=cfg.dropout))\n",
    "        self.fusion = nn.Sequential(*blocks)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # ------- Heads -------\n",
    "        def reg_head():\n",
    "            return nn.Sequential(\n",
    "                nn.LayerNorm(self.D),\n",
    "                nn.Linear(self.D, self.D // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(cfg.dropout),\n",
    "                nn.Linear(self.D // 2, 1),\n",
    "            )\n",
    "\n",
    "        def ratio_head():\n",
    "            return nn.Sequential(\n",
    "                nn.LayerNorm(self.D),\n",
    "                nn.Linear(self.D, self.D // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(cfg.dropout),\n",
    "                nn.Linear(self.D // 2, 1),\n",
    "            )\n",
    "\n",
    "        # direct regressors (raw -> softplus)\n",
    "        self.head_green = reg_head()\n",
    "        self.head_gdm   = reg_head()\n",
    "        self.head_total = reg_head()\n",
    "        self.head_dead  = reg_head()    # direct dead (optional but useful)\n",
    "        self.head_clover= reg_head()    # direct clover (optional)\n",
    "\n",
    "        # ratio heads\n",
    "        self.head_dead_ratio   = ratio_head()\n",
    "        self.head_clover_ratio = ratio_head()\n",
    "\n",
    "        # learnable mixing (logits)\n",
    "        # sigmoid(mix_logit) = weight on direct prediction\n",
    "        dead_mix0, clover_mix0 = cfg.init_mix_logits\n",
    "        self.dead_mix_logit   = nn.Parameter(torch.tensor(float(dead_mix0)))\n",
    "        self.clover_mix_logit = nn.Parameter(torch.tensor(float(clover_mix0)))\n",
    "\n",
    "    def forward(self, left: torch.Tensor, right: torch.Tensor) -> dict:\n",
    "        # tokens: (B, N, D)\n",
    "        if self.cfg.freeze_backbone:\n",
    "            with torch.no_grad():\n",
    "                x_l = self.backbone(left)\n",
    "                x_r = self.backbone(right)\n",
    "        else:\n",
    "            x_l = self.backbone(left)\n",
    "            x_r = self.backbone(right)\n",
    "\n",
    "        # concat token sequences: (B, 2N, D)\n",
    "        x = torch.cat([x_l, x_r], dim=1)\n",
    "\n",
    "        # fuse\n",
    "        x = self.fusion(x)  # (B, 2N, D)\n",
    "\n",
    "        # pool: (B, D)\n",
    "        x_pool = self.pool(x.transpose(1, 2)).squeeze(-1)\n",
    "\n",
    "        # --- direct preds ---\n",
    "        green = F.softplus(self.head_green(x_pool).squeeze(1))\n",
    "        gdm_d = F.softplus(self.head_gdm(x_pool).squeeze(1))\n",
    "        total_d= F.softplus(self.head_total(x_pool).squeeze(1))\n",
    "        dead_d = F.softplus(self.head_dead(x_pool).squeeze(1))\n",
    "        clover_d=F.softplus(self.head_clover(x_pool).squeeze(1))\n",
    "\n",
    "        # --- ratio preds ---\n",
    "        dead_ratio = torch.sigmoid(self.head_dead_ratio(x_pool).squeeze(1))\n",
    "        clover_ratio = torch.sigmoid(self.head_clover_ratio(x_pool).squeeze(1))\n",
    "\n",
    "        # --- derived candidates (two options) ---\n",
    "        dead_from_core   = F.relu(total_d - gdm_d)     # ensures >=0\n",
    "        clover_from_core = F.relu(gdm_d - green)       # ensures >=0\n",
    "\n",
    "        dead_from_ratio  = dead_ratio * total_d\n",
    "        clover_from_ratio= clover_ratio * gdm_d\n",
    "\n",
    "        # pick which derived style you want:\n",
    "        # - core-derived enforces equations and avoids \"invisible dead\"\n",
    "        # - ratio-derived keeps your ratio heads meaningful\n",
    "        # You can also blend core-derived and ratio-derived; keeping it simple:\n",
    "        dead_derived = 0.5 * dead_from_core + 0.5 * dead_from_ratio\n",
    "        clover_derived = 0.5 * clover_from_core + 0.5 * clover_from_ratio\n",
    "\n",
    "        # --- mix direct vs derived ---\n",
    "        mix_dead = torch.sigmoid(self.dead_mix_logit)       # scalar\n",
    "        mix_clover = torch.sigmoid(self.clover_mix_logit)   # scalar\n",
    "\n",
    "        dead = mix_dead * dead_d + (1.0 - mix_dead) * dead_derived\n",
    "        clover = mix_clover * clover_d + (1.0 - mix_clover) * clover_derived\n",
    "\n",
    "        # recompose for consistency (optional)\n",
    "        # If you want strict: force gdm = green + clover, total = gdm + dead\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "\n",
    "        pred5 = torch.stack([green, dead, clover, gdm, total], dim=1)\n",
    "\n",
    "        return {\n",
    "            \"green_pred\": green,\n",
    "            \"gdm_pred\": gdm,\n",
    "            \"total_pred\": total,\n",
    "            \"dead_pred\": dead,\n",
    "            \"clover_pred\": clover,\n",
    "            \"dead_ratio_pred\": dead_ratio,\n",
    "            \"clover_ratio_pred\": clover_ratio,\n",
    "            \"mix_dead\": mix_dead.detach(),\n",
    "            \"mix_clover\": mix_clover.detach(),\n",
    "            \"pred5\": pred5,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30e518",
   "metadata": {
    "papermill": {
     "duration": 0.003831,
     "end_time": "2026-01-27T22:17:12.687399",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.683568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb92e67f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:17:12.696547Z",
     "iopub.status.busy": "2026-01-27T22:17:12.696140Z",
     "iopub.status.idle": "2026-01-27T22:17:12.729692Z",
     "shell.execute_reply": "2026-01-27T22:17:12.728885Z"
    },
    "papermill": {
     "duration": 0.039986,
     "end_time": "2026-01-27T22:17:12.731172",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.691186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Your Dataset stays same (no changes)\n",
    "# -----------------------------------------------------------------------------------\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, img_size: int):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.S = int(img_size)\n",
    "\n",
    "        self.H = self.S * 2\n",
    "        self.W = self.S * 4\n",
    "\n",
    "        self.aug = A.Compose([\n",
    "            A.Resize(self.H, self.W),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = str(row[\"image_path\"])\n",
    "\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            img = np.array(Image.open(f).convert(\"RGB\"))\n",
    "\n",
    "        img_t = self.aug(image=img)[\"image\"]  # (3,H,W)\n",
    "        image_id = row[\"image_id\"]\n",
    "        return image_id, img_t\n",
    "\n",
    "\n",
    "def load_wide_train(train_csv: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(train_csv)\n",
    "    df[\"image_id\"] = df[\"image_path\"].apply(lambda p: os.path.splitext(os.path.basename(p))[0])\n",
    "\n",
    "    wide = (\n",
    "        df.pivot_table(\n",
    "            index=[\"image_id\", \"image_path\", \"Sampling_Date\", \"State\", \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"],\n",
    "            columns=\"target_name\",\n",
    "            values=\"target\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    needed = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "    for c in needed:\n",
    "        if c not in wide.columns:\n",
    "            raise ValueError(f\"Missing target column after pivot: {c}\")\n",
    "\n",
    "    return wide\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Corrected inference with proper ensemble + plotting\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def _nonneg_np(x: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(x, 0.0)\n",
    "\n",
    "def recompute_from_parts_np(green: np.ndarray, dead: np.ndarray, clover: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Enforce exact identities:\n",
    "      GDM = green + clover\n",
    "      Total = GDM + dead\n",
    "    Returns pred5 (N,5) in TARGETS order.\n",
    "    \"\"\"\n",
    "    green = _nonneg_np(green)\n",
    "    dead = _nonneg_np(dead)\n",
    "    clover = _nonneg_np(clover)\n",
    "    gdm = green + clover\n",
    "    total = gdm + dead\n",
    "    return np.stack([green, dead, clover, gdm, total], axis=1).astype(np.float32)\n",
    "\n",
    "def apply_postprocess_from_calib(preds_df: pd.DataFrame, calib: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    preds_df must have columns: image_id + TARGETS\n",
    "    calib is loaded from best_calibration.json (or dict with same keys)\n",
    "    \"\"\"\n",
    "    TARGETS = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "    df = preds_df.copy()\n",
    "\n",
    "    a = np.asarray(calib[\"calib_a\"], dtype=np.float32)\n",
    "    b = np.asarray(calib[\"calib_b\"], dtype=np.float32)\n",
    "    lo = np.asarray(calib[\"clip_lo\"], dtype=np.float32)\n",
    "    hi = np.asarray(calib[\"clip_hi\"], dtype=np.float32)\n",
    "\n",
    "    pred5 = df[TARGETS].to_numpy(np.float32)\n",
    "\n",
    "    # 1) non-neg\n",
    "    pred5 = _nonneg_np(pred5)\n",
    "\n",
    "    # 2) linear calib\n",
    "    pred5 = pred5 * a.reshape(1, 5) + b.reshape(1, 5)\n",
    "\n",
    "    # 3) non-neg again\n",
    "    pred5 = _nonneg_np(pred5)\n",
    "\n",
    "    # 4) clip\n",
    "    pred5 = np.clip(pred5, lo.reshape(1, 5), hi.reshape(1, 5))\n",
    "\n",
    "    # 5) recompute exact sums from parts (green, dead, clover)\n",
    "    pred5 = recompute_from_parts_np(pred5[:, 0], pred5[:, 1], pred5[:, 2])\n",
    "\n",
    "    df[TARGETS] = pred5\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Corrected inference with proper ensemble + plotting\n",
    "# -----------------------------------------------------------------------------------\n",
    "def inference(\n",
    "    model_id: str,\n",
    "    patch_size: int,\n",
    "    model_name: str,\n",
    "    image_paths: list[Path],\n",
    "    weight_paths: list[Path],\n",
    "    model_weights: list[float] | None,\n",
    "    submission_mode: bool,\n",
    "    device: torch.device,\n",
    "    img_size: int,\n",
    "    use_tta: bool,\n",
    "    apply_post_proc: bool,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # -----------------------------\n",
    "    # Validate / default weights\n",
    "    # -----------------------------\n",
    "    if model_weights is None:\n",
    "        model_weights = [1.0 / len(weight_paths)] * len(weight_paths)\n",
    "\n",
    "    if len(model_weights) != len(weight_paths):\n",
    "        raise ValueError(f\"model_weights len {len(model_weights)} != weight_paths len {len(weight_paths)}\")\n",
    "\n",
    "    w_sum = float(sum(model_weights))\n",
    "    if w_sum <= 0:\n",
    "        raise ValueError(\"Sum of model_weights must be > 0\")\n",
    "\n",
    "    # Normalize weights\n",
    "    model_weights = [float(w) / w_sum for w in model_weights]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build inference dataframe\n",
    "    # -----------------------------\n",
    "    rows = [{\"image_id\": p.stem, \"image_path\": str(p)} for p in image_paths]\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    ds = InferenceDataset(df=df, img_size=img_size)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=ds,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    print(f\"Loaded inference data loader: {len(ds)} images\")\n",
    "\n",
    "    labels = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Accumulator for weighted ensemble\n",
    "    # image_id -> np.array(5,)\n",
    "    # -----------------------------\n",
    "    accum: dict[str, np.ndarray] = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fold_idx, (weight_path, w) in enumerate(zip(weight_paths, model_weights)):\n",
    "            print(f\"\\n[Fold {fold_idx}] Loading weights: {weight_path} (weight={w:.4f})\")\n",
    "\n",
    "            # -----------------------------\n",
    "            # Instantiate model ONCE\n",
    "            # -----------------------------\n",
    "            if model_name == \"Dinov3MultiReg\":\n",
    "                model = Dinov3MultiReg(Dinov3Config())\n",
    "            elif model_name == \"DinoV3Structured\":\n",
    "                model = DinoV3Structured(DinoV3StructuredConfig())\n",
    "            elif model_name == \"DinoV3Hybrid\":\n",
    "                model = DinoV3Hybrid(DinoV3HybridConfig())\n",
    "            else:\n",
    "                raise ValueError(f\"model name not found: {model_name}\")\n",
    "        \n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            ckpt = torch.load(weight_path, map_location=device)\n",
    "            state = ckpt[\"model_state\"] if isinstance(ckpt, dict) and \"model_state\" in ckpt else ckpt\n",
    "            model.load_state_dict(state, strict=True)\n",
    "\n",
    "            def forward_pred5(img_full_t: torch.Tensor) -> torch.Tensor:\n",
    "                if model_name == \"DinoV3Structured\":\n",
    "                    out = model(img_full_t)\n",
    "                    return out[\"pred5\"]\n",
    "                elif model_name == \"DinoV3Hybrid\":\n",
    "                    _, _, _, W = img_full_t.shape\n",
    "                    mid = W // 2\n",
    "                    left = img_full_t[:, :, :, :mid]\n",
    "                    right = img_full_t[:, :, :, mid:]\n",
    "                    out = model(left, right)\n",
    "                    return out[\"pred5\"]\n",
    "                else:\n",
    "                    return model(img_full_t)\n",
    "            \n",
    "            pbar = tqdm(data_loader, desc=f\"Inference fold {fold_idx}\", dynamic_ncols=True)\n",
    "            for img_ids, img_full_t in pbar:\n",
    "                img_full_t = img_full_t.to(device, non_blocking=True)\n",
    "\n",
    "                preds = forward_pred5(img_full_t)\n",
    "\n",
    "                if use_tta:\n",
    "                    preds_h = forward_pred5(torch.flip(img_full_t, dims=[3]))\n",
    "                    preds_v = forward_pred5(torch.flip(img_full_t, dims=[2]))\n",
    "                    preds = 0.5 * preds + 0.25 * preds_h + 0.25 * preds_v\n",
    "\n",
    "                preds_np = preds.detach().cpu().numpy()  # (B,5)\n",
    "\n",
    "                for img_id, pred in zip(img_ids, preds_np):\n",
    "                    pred = np.maximum(pred.astype(np.float32), 0.0)  # clamp non-negative\n",
    "\n",
    "                    # weighted accumulate\n",
    "                    if img_id not in accum:\n",
    "                        accum[img_id] = w * pred\n",
    "                    else:\n",
    "                        accum[img_id] += w * pred\n",
    "\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Optional warning if something went missing\n",
    "    if len(accum) != len(ds):\n",
    "        print(f\"⚠️ Warning: accum has {len(accum)} ids but dataset has {len(ds)}. Check duplicate/missing image_id.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build final ensembled preds_df\n",
    "    # -----------------------------\n",
    "    out_rows = []\n",
    "    for img_id, vec in accum.items():\n",
    "        out_rows.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"Dry_Green_g\":  float(vec[0]),\n",
    "            \"Dry_Dead_g\":   float(vec[1]),\n",
    "            \"Dry_Clover_g\": float(vec[2]),\n",
    "            \"GDM_g\":        float(vec[3]),\n",
    "            \"Dry_Total_g\":  float(vec[4]),\n",
    "        })\n",
    "\n",
    "    preds_df = pd.DataFrame(out_rows)\n",
    "\n",
    "    # Ensure same order as input list (robust)\n",
    "    order_map = {p.stem: i for i, p in enumerate(image_paths)}\n",
    "    preds_df[\"__order\"] = preds_df[\"image_id\"].map(order_map).fillna(10**12).astype(np.int64)\n",
    "    preds_df = preds_df.sort_values(\"__order\").drop(columns=\"__order\").reset_index(drop=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Plotting on train/val (when not submission)\n",
    "    # -----------------------------\n",
    "    if not submission_mode:\n",
    "        print(\"Plotting results (GT vs Ensemble Preds)\")\n",
    "        train_df = load_wide_train(Config.train_csv_path)\n",
    "        merged_df = train_df.merge(\n",
    "            preds_df,\n",
    "            on=\"image_id\",\n",
    "            how=\"inner\",\n",
    "            suffixes=(\"_gt\", \"_pred\")\n",
    "        )\n",
    "\n",
    "        if len(merged_df) == 0:\n",
    "            print(\"⚠️ merged_df is empty. Check that image_ids match between train_csv and image_paths.\")\n",
    "        else:\n",
    "            sns.set_theme(style=\"whitegrid\")\n",
    "            targets = labels\n",
    "\n",
    "            fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14, 18), constrained_layout=True)\n",
    "\n",
    "            for i, t in enumerate(targets):\n",
    "                gt = merged_df[f\"{t}_gt\"].astype(float).to_numpy()\n",
    "                pred = merged_df[f\"{t}_pred\"].astype(float).to_numpy()  # ✅ FIXED\n",
    "\n",
    "                # Left: Histogram\n",
    "                ax_hist = axes[i, 0]\n",
    "                sns.histplot(gt, bins=40, stat=\"density\", kde=True, alpha=0.45, label=\"GT\", ax=ax_hist)\n",
    "                sns.histplot(pred, bins=40, stat=\"density\", kde=True, alpha=0.45, label=\"Pred\", ax=ax_hist)\n",
    "                ax_hist.set_title(f\"{t} — Distribution\")\n",
    "                ax_hist.set_xlabel(t)\n",
    "                ax_hist.set_ylabel(\"Density\")\n",
    "                ax_hist.legend()\n",
    "\n",
    "                # Right: Scatter + R²\n",
    "                ax_scatter = axes[i, 1]\n",
    "                r2 = r2_score(gt, pred)\n",
    "\n",
    "                sns.scatterplot(x=gt, y=pred, s=25, alpha=0.6, ax=ax_scatter)\n",
    "\n",
    "                mn = min(gt.min(), pred.min())\n",
    "                mx = max(gt.max(), pred.max())\n",
    "                ax_scatter.plot([mn, mx], [mn, mx], \"r--\", linewidth=1)\n",
    "\n",
    "                ax_scatter.set_title(f\"{t} — Pred vs GT (R² = {r2:.4f})\")\n",
    "                ax_scatter.set_xlabel(\"GT\")\n",
    "                ax_scatter.set_ylabel(\"Pred\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Create submission\n",
    "    # -----------------------------\n",
    "    order = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "\n",
    "\n",
    "    # Applying postprocessing\n",
    "    def post_proc_preds_df(preds_df: pd.DataFrame):\n",
    "        params = dict(\n",
    "            DinoV3Hybrid = {\n",
    "              \"best_score\": 0.7583875060081482,\n",
    "              \"cfg\": {\n",
    "                \"use_ratio_recompose\": False,\n",
    "                \"alpha_dead_derive\": 0.0,\n",
    "                \"use_linear_calib\": True,\n",
    "                \"clip_mode\": \"quantile\",\n",
    "                \"q_low\": 0.1,\n",
    "                \"q_high\": 99.9\n",
    "              },\n",
    "              \"calib_a\": [\n",
    "                1.161050796508789,\n",
    "                0.8045432567596436,\n",
    "                1.1122260093688965,\n",
    "                1.0957939624786377,\n",
    "                1.0523895025253296\n",
    "              ],\n",
    "              \"calib_b\": [\n",
    "                -0.6085014343261719,\n",
    "                1.8143501281738281,\n",
    "                -0.6774587631225586,\n",
    "                0.352935791015625,\n",
    "                0.31894683837890625\n",
    "              ],\n",
    "              \"clip_lo\": [\n",
    "                0.0,\n",
    "                0.0,\n",
    "                0.0,\n",
    "                1.1681599617004395,\n",
    "                1.5526399612426758\n",
    "              ],\n",
    "              \"clip_hi\": [\n",
    "                147.78758239746094,\n",
    "                74.90797424316406,\n",
    "                71.10720825195312,\n",
    "                147.78758239746094,\n",
    "                178.72264099121094\n",
    "              ]\n",
    "            },\n",
    "            DinoV3Structured = {\n",
    "              \"best_score\": 0.7661679983139038,\n",
    "              \"cfg\": {\n",
    "                \"use_ratio_recompose\": True,\n",
    "                \"alpha_dead_derive\": 0.7,\n",
    "                \"use_linear_calib\": False,\n",
    "                \"clip_mode\": \"quantile\",\n",
    "                \"q_low\": 0.1,\n",
    "                \"q_high\": 99.9\n",
    "              },\n",
    "              \"calib_a\": [\n",
    "                1.058223843574524,\n",
    "                0.9063701629638672,\n",
    "                0.9845092296600342,\n",
    "                1.0237661600112915,\n",
    "                1.0476360321044922\n",
    "              ],\n",
    "              \"calib_b\": [\n",
    "                0.6819171905517578,\n",
    "                1.508835792541504,\n",
    "                -0.19695329666137695,\n",
    "                1.05670166015625,\n",
    "                0.17140579223632812\n",
    "              ],\n",
    "              \"clip_lo\": [\n",
    "                0.0,\n",
    "                0.0,\n",
    "                0.0,\n",
    "                1.1681599617004395,\n",
    "                1.5526399612426758\n",
    "              ],\n",
    "              \"clip_hi\": [\n",
    "                147.78758239746094,\n",
    "                74.90797424316406,\n",
    "                71.10720825195312,\n",
    "                147.78758239746094,\n",
    "                178.72264099121094\n",
    "              ]\n",
    "            },\n",
    "        )\n",
    "        calib = params[model_name]\n",
    "        final_preds_df = apply_postprocess_from_calib(preds_df, calib)\n",
    "\n",
    "        if not submission_mode:\n",
    "            assert (preds_df[\"GDM_g\"] - (preds_df[\"Dry_Green_g\"] + preds_df[\"Dry_Clover_g\"])).abs().max() < 1e-3\n",
    "            assert (preds_df[\"Dry_Total_g\"] - (preds_df[\"GDM_g\"] + preds_df[\"Dry_Dead_g\"])).abs().max() < 1e-3\n",
    "\n",
    "        return final_preds_df\n",
    "\n",
    "\n",
    "    if apply_post_proc:\n",
    "        preds_df = post_proc_preds_df(preds_df)\n",
    "    \n",
    "    submission_rows = []\n",
    "    for _, row in preds_df.iterrows():\n",
    "        sample_id = row[\"image_id\"]\n",
    "        for target_name in order:\n",
    "            submission_rows.append({\n",
    "                \"sample_id\": f\"{sample_id}__{target_name}\",\n",
    "                \"target\": float(row[target_name]),\n",
    "            })\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_rows, columns=[\"sample_id\", \"target\"])\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Saved results to submission.csv\")\n",
    "\n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91974362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:17:12.740993Z",
     "iopub.status.busy": "2026-01-27T22:17:12.740399Z",
     "iopub.status.idle": "2026-01-27T22:20:29.404054Z",
     "shell.execute_reply": "2026-01-27T22:20:29.403187Z"
    },
    "papermill": {
     "duration": 196.670367,
     "end_time": "2026-01-27T22:20:29.405602",
     "exception": false,
     "start_time": "2026-01-27T22:17:12.735235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "Loaded inference data loader: 1 images\n",
      "\n",
      "[Fold 0] Loading weights: /kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/2776e2c07fd544d79afcbfff8db8f429/artifacts/fold_0_best/DinoV3Hybrid_fold0.pt (weight=0.2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference fold 0: 100%|██████████| 1/1 [00:04<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 1] Loading weights: /kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/6a52b6a027b74fc0b65234a84fe50cc8/artifacts/fold_1_best/DinoV3Hybrid_fold1.pt (weight=0.2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference fold 1: 100%|██████████| 1/1 [00:03<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 2] Loading weights: /kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/b3a849cab7954bf5a98e62ab3d60964f/artifacts/fold_2_best/DinoV3Hybrid_fold2.pt (weight=0.2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference fold 2: 100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 3] Loading weights: /kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/9567cd0f28af4b8d9d407df1c30ddb1d/artifacts/fold_3_best/DinoV3Hybrid_fold3.pt (weight=0.2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference fold 3: 100%|██████████| 1/1 [00:03<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 4] Loading weights: /kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/7085174f4dc9463f9ee7c349453368e1/artifacts/fold_4_best/DinoV3Hybrid_fold4.pt (weight=0.2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference fold 4: 100%|██████████| 1/1 [00:03<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1001187975__Dry_Clover_g</td>\n",
       "      <td>0.624027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1001187975__Dry_Dead_g</td>\n",
       "      <td>25.299662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1001187975__Dry_Green_g</td>\n",
       "      <td>23.919390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1001187975__Dry_Total_g</td>\n",
       "      <td>49.843082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001187975__GDM_g</td>\n",
       "      <td>24.543417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id     target\n",
       "0  ID1001187975__Dry_Clover_g   0.624027\n",
       "1    ID1001187975__Dry_Dead_g  25.299662\n",
       "2   ID1001187975__Dry_Green_g  23.919390\n",
       "3   ID1001187975__Dry_Total_g  49.843082\n",
       "4         ID1001187975__GDM_g  24.543417"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing cache and reduce RAM\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "test_image_paths = [img_path for img_path in Config.test_dir.glob(\"*\")]\n",
    "submission_mode = len(test_image_paths) >= 1\n",
    "\n",
    "if submission_mode:\n",
    "    image_paths = [Path(img_path) for img_path in Config.test_dir.glob(\"*\")]\n",
    "else:\n",
    "    image_paths = [Path(img_path) for img_path in Config.train_dir.glob(\"*\")][:10]\n",
    "\n",
    "# # Dinov3 mamba weights 8 tiles\n",
    "# ----------------------------------\n",
    "# model_local_cv = [\n",
    "#     0.6726178856038335,\n",
    "#     0.6315996830280011,\n",
    "#     0.7813904246726593,\n",
    "#     0.7476579813247032,\n",
    "#     0.717584684713563,\n",
    "# ]\n",
    "\n",
    "# model_lb_score = [\n",
    "#     0.66,\n",
    "#     0.68,\n",
    "#     0.66,\n",
    "#     0.65,\n",
    "#     0.62\n",
    "# ]\n",
    "\n",
    "# Dinvo3 mamba weights 2 tiles\n",
    "# ----------------------------------\n",
    "model_local_cv = [\n",
    "    0.681189797707458,\n",
    "    0.6198682876733633,\n",
    "    0.751599745316939,\n",
    "    0.7379660302020133,\n",
    "    0.7346212232290809,\n",
    "]\n",
    "\n",
    "# model_weights = [score/sum(model_local_cv) for score in model_local_cv]\n",
    "model_weights = [1/len(model_local_cv) for score in model_local_cv]\n",
    "print(model_weights)\n",
    "\n",
    "submission_df_dinov3 = inference(\n",
    "    model_id = \"vit_huge_plus_patch16_dinov3.lvd1689m\",\n",
    "    patch_size = 16,\n",
    "    model_name = \"DinoV3Hybrid\",\n",
    "    image_paths = image_paths,\n",
    "    weight_paths = [\n",
    "        # Dinov3-mamba-8-tiles-approach\n",
    "        # Path(\"/kaggle/input/csiro-dinov3-mamba-5-fold-ensemble/30bd90d9235742d098ef18efced22c98/artifacts/fold_0_best/DinoV3Structured_fold0.pt\"),\n",
    "        # Path(\"/kaggle/input/csiro-dinov3-mamba-5-fold-ensemble/dee2f45859bb41c88e29727b07c96726/artifacts/fold_1_best/DinoV3Structured_fold1.pt\"),\n",
    "        # Path(\"/kaggle/input/csiro-dinov3-mamba-5-fold-ensemble/9daf136ace1f41029e4b0be4e8ee6229/artifacts/fold_2_best/DinoV3Structured_fold2.pt\"),\n",
    "        # Path(\"/kaggle/input/csiro-dinov3-mamba-5-fold-ensemble/536f0aad17dc4e9d93770f5b30d9d72d/artifacts/fold_3_best/DinoV3Structured_fold3.pt\"),\n",
    "        # Path(\"/kaggle/input/csiro-dinov3-mamba-5-fold-ensemble/artifacts/fold_4_best/DinoV3Structured_fold4.pt\"),\n",
    "\n",
    "\n",
    "        # Dinov3 mamba 2 tiles approach\n",
    "        \"/kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/2776e2c07fd544d79afcbfff8db8f429/artifacts/fold_0_best/DinoV3Hybrid_fold0.pt\",\n",
    "        \"/kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/6a52b6a027b74fc0b65234a84fe50cc8/artifacts/fold_1_best/DinoV3Hybrid_fold1.pt\",\n",
    "        \"/kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/b3a849cab7954bf5a98e62ab3d60964f/artifacts/fold_2_best/DinoV3Hybrid_fold2.pt\",\n",
    "        \"/kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/9567cd0f28af4b8d9d407df1c30ddb1d/artifacts/fold_3_best/DinoV3Hybrid_fold3.pt\",\n",
    "        \"/kaggle/input/dinov3-mamba-gdm-green-total-ratio-2-tile-approach/7085174f4dc9463f9ee7c349453368e1/artifacts/fold_4_best/DinoV3Hybrid_fold4.pt\",\n",
    "        \n",
    "    ],\n",
    "    model_weights = model_weights,\n",
    "    submission_mode = submission_mode,\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    img_size = 512,\n",
    "    use_tta = False,\n",
    "    apply_post_proc = False,\n",
    ")\n",
    "\n",
    "submission_df_dinov3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bb261",
   "metadata": {
    "papermill": {
     "duration": 0.005,
     "end_time": "2026-01-27T22:20:29.415784",
     "exception": false,
     "start_time": "2026-01-27T22:20:29.410784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Classical preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3f05fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:20:29.427481Z",
     "iopub.status.busy": "2026-01-27T22:20:29.426923Z",
     "iopub.status.idle": "2026-01-27T22:20:58.067590Z",
     "shell.execute_reply": "2026-01-27T22:20:58.066915Z"
    },
    "papermill": {
     "duration": 28.648505,
     "end_time": "2026-01-27T22:20:58.069102",
     "exception": false,
     "start_time": "2026-01-27T22:20:29.420597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33494588609703535, 0.33254360895014495, 0.33251050495281975]\n",
      "[Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('pca', PCA(n_components=64, random_state=42)),\n",
      "                ('model', Ridge(alpha=np.float64(316.2277660168379)))]),\n",
      " Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('pca', PCA(n_components=64, random_state=42)),\n",
      "                ('model',\n",
      "                 ElasticNet(alpha=0.003, l1_ratio=0.8, max_iter=50000))]),\n",
      " Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('pca', PCA(n_components=64, random_state=42)),\n",
      "                ('model', LinearRegression())])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0450d5e6bf446a85e32ce9e275b6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to submission.csv\n",
      "Final preds min per target: {'Dry_Green_g': 28.218172073364258, 'Dry_Clover_g': 1.704144835472107, 'Dry_Dead_g': 27.984580993652344, 'GDM_g': 29.922317504882812, 'Dry_Total_g': 57.906898498535156}\n",
      "Final preds max per target: {'Dry_Green_g': 28.218172073364258, 'Dry_Clover_g': 1.704144835472107, 'Dry_Dead_g': 27.984580993652344, 'GDM_g': 29.922317504882812, 'Dry_Total_g': 57.906898498535156}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1001187975__Dry_Clover_g</td>\n",
       "      <td>1.704145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1001187975__Dry_Dead_g</td>\n",
       "      <td>27.984581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1001187975__Dry_Green_g</td>\n",
       "      <td>28.218172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1001187975__Dry_Total_g</td>\n",
       "      <td>57.906898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001187975__GDM_g</td>\n",
       "      <td>29.922318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id     target\n",
       "0  ID1001187975__Dry_Clover_g   1.704145\n",
       "1    ID1001187975__Dry_Dead_g  27.984581\n",
       "2   ID1001187975__Dry_Green_g  28.218172\n",
       "3   ID1001187975__Dry_Total_g  57.906898\n",
       "4         ID1001187975__GDM_g  29.922318"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import joblib\n",
    "import json\n",
    "import timm\n",
    "import random\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import Ridge, ElasticNet, LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Mass balance projection\n",
    "# ---------------------------\n",
    "def apply_mass_balance_projection(pred: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Enforce:\n",
    "      1) Green + Clover - GDM = 0\n",
    "      2) GDM + Dead - Total = 0\n",
    "\n",
    "    pred: (N, 5) in order [Green, Clover, Dead, GDM, Total]\n",
    "    Returns corrected pred (N, 5) (closest in L2 sense).\n",
    "    \"\"\"\n",
    "    x = pred.astype(np.float64)\n",
    "\n",
    "    # A x = 0\n",
    "    A = np.array([\n",
    "        [1.0, 1.0, 0.0, -1.0, 0.0],   # Green + Clover - GDM = 0\n",
    "        [0.0, 0.0, 1.0,  1.0, -1.0],  # Dead + GDM - Total = 0\n",
    "    ], dtype=np.float64)  # (2,5)\n",
    "\n",
    "    # Projection: x' = x - A^T (A A^T)^-1 (A x)\n",
    "    AA_T = A @ A.T  # (2,2)\n",
    "    inv = np.linalg.inv(AA_T)\n",
    "\n",
    "    # (N,2) residual per sample\n",
    "    r = (A @ x.T).T\n",
    "    correction = (A.T @ (inv @ r.T)).T  # (N,5)\n",
    "    x_corr = x - correction\n",
    "\n",
    "    return x_corr.astype(np.float32)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# DINOv3 offline loader\n",
    "# ---------------------------\n",
    "def build_dinov3_backbone_offline(model_id: str, img_size: int, device: str, weights_dir: Path):\n",
    "    weights_path = weights_dir / f\"{model_id}.pt\"\n",
    "    meta_path = weights_dir / f\"{model_id}.json\"\n",
    "\n",
    "    assert weights_path.exists(), f\"Missing weights: {weights_path}\"\n",
    "    assert meta_path.exists(), f\"Missing meta: {meta_path}\"\n",
    "\n",
    "    meta = json.loads(meta_path.read_text())\n",
    "\n",
    "    model = timm.create_model(meta[\"model_id\"], pretrained=False, num_classes=meta[\"num_classes\"])\n",
    "    sd = torch.load(weights_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval().to(device)\n",
    "\n",
    "    cfg = timm.data.resolve_model_data_config(model)\n",
    "    cfg[\"input_size\"] = (3, img_size, img_size)\n",
    "    transform = timm.data.create_transform(**cfg, is_training=False)\n",
    "\n",
    "    return model, transform\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Embedding extraction\n",
    "# ---------------------------\n",
    "@torch.inference_mode()\n",
    "def extract_embeddings_from_paths(\n",
    "    image_paths: list[Path],\n",
    "    model,\n",
    "    transform,\n",
    "    device: str,\n",
    "    batch_size: int = 8,\n",
    ") -> np.ndarray:\n",
    "    embs = []\n",
    "    batch = []\n",
    "\n",
    "    for i, p in enumerate(tqdm(image_paths, desc=\"Extracting embeddings\")):\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        x = transform(img)  # (C,H,W)\n",
    "        batch.append(x)\n",
    "\n",
    "        if len(batch) == batch_size or (i == len(image_paths) - 1):\n",
    "            bx = torch.stack(batch, dim=0).to(device, non_blocking=True)\n",
    "            feats = model(bx)\n",
    "\n",
    "            # timm may return (B,D) or (B,T,D)\n",
    "            if feats.ndim == 3:\n",
    "                feats = feats[:, 0, :]  # CLS\n",
    "\n",
    "            embs.append(feats.detach().float().cpu().numpy())\n",
    "            batch = []\n",
    "\n",
    "    return np.concatenate(embs, axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Ensemble helpers\n",
    "# ---------------------------\n",
    "def ensemble_equal(preds_list: list[np.ndarray]) -> np.ndarray:\n",
    "    return np.mean(np.stack(preds_list, axis=0), axis=0)\n",
    "\n",
    "def ensemble_weighted(preds_list: list[np.ndarray], weights: list[float]) -> np.ndarray:\n",
    "    w = np.asarray(weights, dtype=np.float64)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    out = np.zeros_like(preds_list[0], dtype=np.float64)\n",
    "    for wi, pi in zip(w, preds_list):\n",
    "        out += wi * pi.astype(np.float64)\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "def load_wide_train(train_csv: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(train_csv)\n",
    "    df[\"image_id\"] = df[\"image_path\"].apply(lambda p: os.path.splitext(os.path.basename(p))[0])\n",
    "\n",
    "    wide = (\n",
    "        df.pivot_table(\n",
    "            index=[\"image_id\", \"image_path\", \"Sampling_Date\", \"State\", \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"],\n",
    "            columns=\"target_name\",\n",
    "            values=\"target\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    needed = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "    for c in needed:\n",
    "        if c not in wide.columns:\n",
    "            raise ValueError(f\"Missing target column after pivot: {c}\")\n",
    "\n",
    "    return wide\n",
    "\n",
    "\n",
    "def plot_gt_vs_pred(merged_df, target_names, title=\"\"):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    n = len(target_names)\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=2, figsize=(14, 3.5 * n), constrained_layout=True)\n",
    "    if n == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    for i, t in enumerate(target_names):\n",
    "        gt = merged_df[f\"{t}_gt\"].to_numpy(dtype=np.float64)\n",
    "        pred = merged_df[f\"{t}_pred\"].to_numpy(dtype=np.float64)\n",
    "\n",
    "        mask = np.isfinite(gt) & np.isfinite(pred)\n",
    "        gt = gt[mask]\n",
    "        pred = pred[mask]\n",
    "\n",
    "        ax_hist = axes[i, 0]\n",
    "        ax_scatter = axes[i, 1]\n",
    "\n",
    "        if len(gt) == 0:\n",
    "            ax_hist.set_title(f\"{t} — EMPTY (check merge)\")\n",
    "            ax_scatter.set_title(f\"{t} — EMPTY\")\n",
    "            continue\n",
    "\n",
    "        # Histogram\n",
    "        sns.histplot(gt, bins=40, stat=\"density\", kde=True, alpha=0.45, label=\"GT\", ax=ax_hist)\n",
    "        sns.histplot(pred, bins=40, stat=\"density\", kde=True, alpha=0.45, label=\"Pred\", ax=ax_hist)\n",
    "        ax_hist.legend()\n",
    "        ax_hist.set_title(f\"{t} — Distribution\")\n",
    "\n",
    "        # Scatter\n",
    "        r2 = r2_score(gt, pred)\n",
    "        sns.scatterplot(x=gt, y=pred, s=25, alpha=0.6, ax=ax_scatter)\n",
    "\n",
    "        mn = min(gt.min(), pred.min())\n",
    "        mx = max(gt.max(), pred.max())\n",
    "        ax_scatter.plot([mn, mx], [mn, mx], \"r--\", linewidth=1)\n",
    "\n",
    "        ax_scatter.set_title(f\"{t} — Pred vs GT (R² = {r2:.4f})\")\n",
    "        ax_scatter.set_xlabel(\"GT\")\n",
    "        ax_scatter.set_ylabel(\"Pred\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Main inference\n",
    "# ---------------------------\n",
    "def inference_classical(\n",
    "    image_paths: list[Path],\n",
    "    submission_mode: bool,\n",
    "    apply_mass_balance: bool,\n",
    "    model_id: str,  # timm id (NOT .json)\n",
    "    embedding_model_weights_dir: str,\n",
    "    classical_models: list,          # list of sklearn pipelines loaded via joblib\n",
    "    model_weights: list[float] | None,\n",
    "    device: str,\n",
    "    batch_size: int = 8,\n",
    "    img_size: int = 512,\n",
    "    out_csv: str = \"submission.csv\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    End-to-end inference:\n",
    "      1) Load DINOv3 embedding model offline (.pt + .json meta)\n",
    "      2) Extract embeddings for image_paths\n",
    "      3) Predict with multiple classical models\n",
    "      4) Build equal + weighted ensemble\n",
    "      5) (Optional) non-negativity + mass-balance projection\n",
    "      6) Write competition submission.csv in long format: sample_id__target_name, target\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # small helper: postprocess\n",
    "    # ---------------------------\n",
    "    def postprocess_preds(pred: np.ndarray, do_mb: bool) -> np.ndarray:\n",
    "        pred = np.asarray(pred, dtype=np.float32)\n",
    "\n",
    "        # masses can't be negative (models can output negatives)\n",
    "        pred = np.maximum(pred, 0.0)\n",
    "\n",
    "        # enforce constraints if requested\n",
    "        if do_mb:\n",
    "            pred = apply_mass_balance_projection(pred).astype(np.float32)\n",
    "\n",
    "        # projection can introduce tiny negatives again\n",
    "        pred = np.maximum(pred, 0.0)\n",
    "        return pred\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1) load embedding model offline\n",
    "    # ---------------------------\n",
    "    weights_dir = Path(embedding_model_weights_dir)\n",
    "    emb_model, transform = build_dinov3_backbone_offline(\n",
    "        model_id=model_id,\n",
    "        img_size=img_size,\n",
    "        device=device,\n",
    "        weights_dir=weights_dir,\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: this must match your mass-balance A-matrix ordering:\n",
    "    # [Green, Clover, Dead, GDM, Total]\n",
    "    target_names = [\"Dry_Green_g\", \"Dry_Clover_g\", \"Dry_Dead_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2) embeddings\n",
    "    # ---------------------------\n",
    "    X = extract_embeddings_from_paths(\n",
    "        image_paths=image_paths,\n",
    "        model=emb_model,\n",
    "        transform=transform,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3) predict per classical model\n",
    "    # ---------------------------\n",
    "    preds_list = []\n",
    "    for m in classical_models:\n",
    "        pred = m.predict(X).astype(np.float32)  # (N, 5)\n",
    "        if pred.ndim == 1:\n",
    "            raise ValueError(\"Model returned 1D pred; expected (N,5). Did you forget MultiOutput?\")\n",
    "        if pred.shape[1] != len(target_names):\n",
    "            raise ValueError(f\"Pred shape {pred.shape} does not match targets={len(target_names)}\")\n",
    "        preds_list.append(pred)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4) ensembles\n",
    "    # ---------------------------\n",
    "    pred_equal = ensemble_equal(preds_list)\n",
    "\n",
    "    if model_weights is None:\n",
    "        pred_weighted = pred_equal.copy()\n",
    "    else:\n",
    "        pred_weighted = ensemble_weighted(preds_list, model_weights)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5) postprocess (clip negatives + optional mass balance)\n",
    "    # ---------------------------\n",
    "    pred_equal = postprocess_preds(pred_equal, do_mb=apply_mass_balance)\n",
    "    pred_weighted = postprocess_preds(pred_weighted, do_mb=apply_mass_balance)\n",
    "\n",
    "    # choose final predictions for submission (weighted usually best)\n",
    "    final_pred = pred_weighted\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6) build wide preds df (optional debug)\n",
    "    # ---------------------------\n",
    "    ids = [p.stem for p in image_paths]\n",
    "    preds_wide = pd.DataFrame({\"sample_id\": ids})\n",
    "    for i, t in enumerate(target_names):\n",
    "        preds_wide[t] = final_pred[:, i]\n",
    "\n",
    "    # Optional local validation plots (only when not in submission mode)\n",
    "    if not submission_mode:\n",
    "        try:\n",
    "            train_df = load_wide_train(Config.train_csv_path)\n",
    "            merged_df = train_df.merge(\n",
    "                preds_wide,\n",
    "                left_on=\"image_id\",\n",
    "                right_on=\"sample_id\",\n",
    "                how=\"inner\",\n",
    "                suffixes=(\"_gt\", \"_pred\")\n",
    "            )\n",
    "            \n",
    "            print(\"Merged rows:\", len(merged_df))\n",
    "            \n",
    "            plot_gt_vs_pred(\n",
    "                merged_df,\n",
    "                target_names,\n",
    "                title=\"Weighted Ensemble (after Mass Balance)\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping debug plots due to: {e}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7) Create competition submission (long format)\n",
    "    # -----------------------------\n",
    "    # Competition expects this order:\n",
    "    order = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "\n",
    "    submission_rows = []\n",
    "    for _, row in preds_wide.iterrows():\n",
    "        sid = row[\"sample_id\"]\n",
    "        for target_name in order:\n",
    "            submission_rows.append({\n",
    "                \"sample_id\": f\"{sid}__{target_name}\",\n",
    "                \"target\": float(row[target_name]),\n",
    "            })\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_rows, columns=[\"sample_id\", \"target\"])\n",
    "    submission_df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved results to {out_csv}\")\n",
    "\n",
    "    # quick sanity\n",
    "    print(\"Final preds min per target:\", preds_wide[target_names].min().to_dict())\n",
    "    print(\"Final preds max per target:\", preds_wide[target_names].max().to_dict())\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "# Removing cache and reduce RAM\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "test_image_paths = [img_path for img_path in Config.test_dir.glob(\"*\")]\n",
    "submission_mode = len(test_image_paths) >= 1\n",
    "\n",
    "if submission_mode:\n",
    "    image_paths = [Path(img_path) for img_path in Config.test_dir.glob(\"*\")]\n",
    "else:\n",
    "    image_paths = [Path(img_path) for img_path in Config.train_dir.glob(\"*\")]\n",
    "\n",
    "# Loading classical models\n",
    "model_paths = [\n",
    "    \"/kaggle/input/dinov3-embedding-classical-models/classical_model_results/Ridge_pca64.joblib\",\n",
    "    \"/kaggle/input/dinov3-embedding-classical-models/classical_model_results/ElasticNet_pca64.joblib\",\n",
    "    \"/kaggle/input/dinov3-embedding-classical-models/classical_model_results/LinearRegression_pca64.joblib\",\n",
    "]\n",
    "\n",
    "# Loading best pca param\n",
    "ensemble_metadata = json.load(\n",
    "    open(\n",
    "        \"/kaggle/input/dinov3-embedding-classical-models/classical_model_results/ensemble_metadata.json\", \n",
    "        \"r\"\n",
    "    )\n",
    ")\n",
    "\n",
    "local_cv_scores = [\n",
    "    0.77473443, \n",
    "    0.76917793, \n",
    "    0.76910136\n",
    "]\n",
    "model_weights = [\n",
    "    score/sum(local_cv_scores)\n",
    "    for score in local_cv_scores\n",
    "]\n",
    "\n",
    "# model_weights = [\n",
    "#     1/len(local_cv_scores)\n",
    "#     for score in local_cv_scores\n",
    "# ]\n",
    "\n",
    "print(model_weights)\n",
    "classical_models = [\n",
    "    joblib.load(path)\n",
    "    for path in model_paths\n",
    "]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(classical_models)\n",
    "\n",
    "submission_df_classical = inference_classical(\n",
    "    image_paths=image_paths,\n",
    "    submission_mode=submission_mode,\n",
    "    apply_mass_balance=True,\n",
    "\n",
    "    model_id=\"vit_huge_plus_patch16_dinov3.lvd1689m\",\n",
    "    embedding_model_weights_dir=\"/kaggle/input/dinov3-embedding-classical-models/classical_model_results/embedding_model\",\n",
    "\n",
    "    classical_models=classical_models,\n",
    "    model_weights=model_weights,\n",
    "\n",
    "    device=str(Config.device),\n",
    "    batch_size=8,\n",
    "    img_size=512,\n",
    ")\n",
    "\n",
    "submission_df_classical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32095d65",
   "metadata": {
    "papermill": {
     "duration": 0.005379,
     "end_time": "2026-01-27T22:20:58.080163",
     "exception": false,
     "start_time": "2026-01-27T22:20:58.074784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40da6672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T22:20:58.092569Z",
     "iopub.status.busy": "2026-01-27T22:20:58.091918Z",
     "iopub.status.idle": "2026-01-27T22:20:58.106514Z",
     "shell.execute_reply": "2026-01-27T22:20:58.105866Z"
    },
    "papermill": {
     "duration": 0.022783,
     "end_time": "2026-01-27T22:20:58.108048",
     "exception": false,
     "start_time": "2026-01-27T22:20:58.085265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights used: [0.65 0.35]\n",
      "Saved: submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "lb_scores = [0.71, 0.66]\n",
    "submissions_df = [submission_df_dinov3, submission_df_classical]\n",
    "\n",
    "def ensemble_submissions(sub_dfs, weights=None):\n",
    "    # keep only required cols + ensure float\n",
    "    dfs = []\n",
    "    for df in sub_dfs:\n",
    "        d = df[[\"sample_id\", \"target\"]].copy()\n",
    "        d[\"target\"] = d[\"target\"].astype(np.float64)\n",
    "        d = d.set_index(\"sample_id\")\n",
    "        dfs.append(d)\n",
    "\n",
    "    # join on sample_id (inner ensures same ordering / keys)\n",
    "    joined = pd.concat(dfs, axis=1, join=\"inner\")\n",
    "    joined.columns = [f\"m{i}\" for i in range(len(dfs))]\n",
    "\n",
    "    # sanity checks\n",
    "    if not submission_mode:\n",
    "        assert joined.shape[0] == sub_dfs[0].shape[0], \"Mismatch in sample_id rows across submissions!\"\n",
    "        assert joined.isna().sum().sum() == 0, \"Found NaNs after joining—some sample_ids missing.\"\n",
    "\n",
    "    if weights is None:\n",
    "        w = np.ones(len(dfs), dtype=np.float64) / len(dfs)\n",
    "    else:\n",
    "        w = np.asarray(weights, dtype=np.float64)\n",
    "        w = w / (w.sum() + 1e-12)\n",
    "\n",
    "    # weighted average across columns\n",
    "    ens = joined.values @ w\n",
    "    out = pd.DataFrame({\"sample_id\": joined.index, \"target\": ens.astype(np.float32)})\n",
    "    return out, w\n",
    "\n",
    "# # 1) weighted by LB scores\n",
    "# ens_weighted, w_used = ensemble_submissions(submissions_df, weights=lb_scores)\n",
    "# print(\"Weights used:\", w_used)\n",
    "\n",
    "#  2) Manual weights\n",
    "weights = [0.65, 0.35]\n",
    "ens_weighted, w_used = ensemble_submissions(submissions_df, weights=weights)\n",
    "print(\"Weights used:\", w_used)\n",
    "\n",
    "ens_weighted.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved: submission.csv\")\n",
    "\n",
    "# # 2) equal ensemble (optional)\n",
    "# ens_equal, _ = ensemble_submissions(submissions_df, weights=None)\n",
    "# ens_equal.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"Saved: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261aa2b",
   "metadata": {
    "papermill": {
     "duration": 0.005456,
     "end_time": "2026-01-27T22:20:58.119108",
     "exception": false,
     "start_time": "2026-01-27T22:20:58.113652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "isSourceIdPinned": false,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8918428,
     "sourceId": 13994195,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9293280,
     "sourceId": 14550076,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9296323,
     "sourceId": 14554725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9302562,
     "sourceId": 14563716,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9307372,
     "sourceId": 14570786,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9328751,
     "sourceId": 14604622,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9344616,
     "sourceId": 14628814,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9354066,
     "sourceId": 14643019,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 287.212887,
   "end_time": "2026-01-27T22:21:02.115745",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-27T22:16:14.902858",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07d6b4d734a844b7ac5adcf163d5441f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_97da07e5511a4fba8e8bbd68ce296fe3",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fe1a6769a4f64b13b349ecbecab14ca0",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "0f3e7a21b24f451c82175ef59d2e3575": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39752ea8a3094a23ac259514186f8936": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "401b0a2921d748b091bf780ed5fe476c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8b39814c08344333b1c19177a69545e2",
       "placeholder": "​",
       "style": "IPY_MODEL_86a614cacb404e3aa062adf9b3fede59",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00,  2.31it/s]"
      }
     },
     "473ce18aa3964dd79ad134cb13747282": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "60bf79c98afc4cdbb2a61c3a6cda7865": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_473ce18aa3964dd79ad134cb13747282",
       "placeholder": "​",
       "style": "IPY_MODEL_39752ea8a3094a23ac259514186f8936",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting embeddings: 100%"
      }
     },
     "86a614cacb404e3aa062adf9b3fede59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8b39814c08344333b1c19177a69545e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "97da07e5511a4fba8e8bbd68ce296fe3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df0450d5e6bf446a85e32ce9e275b6d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_60bf79c98afc4cdbb2a61c3a6cda7865",
        "IPY_MODEL_07d6b4d734a844b7ac5adcf163d5441f",
        "IPY_MODEL_401b0a2921d748b091bf780ed5fe476c"
       ],
       "layout": "IPY_MODEL_0f3e7a21b24f451c82175ef59d2e3575",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fe1a6769a4f64b13b349ecbecab14ca0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
